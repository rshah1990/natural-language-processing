{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"**In this assignment you will learn how to predict tags for posts from StackOverflow. To solve this task you will use multilabel classification approach.**\n\nLibraries\nIn this task you will need the following libraries:\n\nNumpy — a package for scientific computing.\n\nPandas — a library providing high-performance, easy-to-use data structures and data analysis tools for the Python\n\nscikit-learn — a tool for data mining and data analysis.\n\nNLTK — a platform to work with natural language."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"### Text preprocessing"},{"metadata":{"_uuid":"af25a85b512058368c6fd4499f7a4d46588d9fa7"},"cell_type":"markdown","source":"For this we will use \"NLTK\" library"},{"metadata":{"trusted":true,"_uuid":"7d707b15ccefb4bf66d7a239991d8a26c04b254c"},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35143ce2f46a06e8f0437b0d9418a598d9527df8"},"cell_type":"markdown","source":"In this task you will deal with a dataset of post titles from StackOverflow. You are provided a split to 3 sets: *train*, *validation* and *test*. All corpora (except for *test*) contain titles of the posts and corresponding tags (100 tags are available). The *test* set is provided for Coursera's grading and doesn't contain answers. Upload the corpora using *pandas* and look at the data:"},{"metadata":{"trusted":true,"_uuid":"a7cb2c80cca2ac5ad6ef8a036c0638579aec3df5"},"cell_type":"code","source":"from ast import literal_eval\nimport pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f3cbbb63f0b05f0a41f6bd5b7aae9d0bf5e832a"},"cell_type":"code","source":"# create a function to read data \ndef read_data(filename):\n    data = pd.read_csv(filename, sep='\\t')\n    data['tags'] = data['tags'].apply(literal_eval)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9841b10497e77e51d978653e8cb5b0ec0f8cff2a"},"cell_type":"code","source":"train = read_data('../input/train.tsv')\nvalidation = read_data('../input/validation.tsv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3120c325a1228ddfa7d620d414c458aa83cae11"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"683d1f5ae5c968794ac024fafbf7fb17a238cef2"},"cell_type":"code","source":"import re\nREPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]') # \nSTOPWORDS = set(stopwords.words('english')) # set of stop word\n\ndef text_prepare(text):\n    \"\"\"\n        text: a string\n        return: modified initial string\n    \"\"\"\n    text = text.lower() # lowercase text\n#     print(text)\n    text = re.sub(REPLACE_BY_SPACE_RE, \" \", text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n#     print(text)\n    text = re.sub(BAD_SYMBOLS_RE, \"\", text) # delete symbols which are in BAD_SYMBOLS_RE from text\n#     print(text)\n    text = \" \" + text + \" \"\n    for sw in STOPWORDS:\n        text = text.replace(\" \"+sw+\" \", \" \") # delete stopwors from text it is important to add space at the start and end of string to be replace \n                                             # else it would replace indivitual alphabate by space \n#     print(text)\n    text = re.sub('[ ][ ]+', \" \", text)\n    if text[0] == ' ':\n        text = text[1:]\n    if text[-1] == ' ':\n        text = text[:-1]\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bbf9e9d9422dc41a87455ed9ce0e65c1afa5766"},"cell_type":"code","source":"def test_text_prepare():\n    examples = [\"SQL Server - any equivalent of Excel's CHOOSE function?\",\n                \"How to free c++ memory vector<int> * arr?\"]\n    answers = [\"sql server equivalent excels choose function\", \n               \"free c++ memory vectorint arr\"]\n    for ex, ans in zip(examples, answers):\n        if text_prepare(ex) != ans:\n            return \"Wrong answer for the case: '%s'\" % ex\n    return 'Basic tests are passed.'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73783efff6ddbbb559b6dba071af38302b230612"},"cell_type":"code","source":"print(test_text_prepare())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"908340c064d20eef36735d62b56e204fa6cf1b32"},"cell_type":"code","source":"X_train, y_train = train['title'].values, train['tags'].values\nX_val, y_val = validation['title'].values, validation['tags'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"989e75903b7dd3bca93b0fc95f78364814985126"},"cell_type":"code","source":"X_train = [text_prepare(x) for x in X_train]\nX_val = [text_prepare(x) for x in X_val]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d4a3ff2a11fedf4e45650b01c5353ff22031918"},"cell_type":"code","source":"X_train[:3]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7aa29d2c9ef0974d606608fa79a72d629c681cf0"},"cell_type":"markdown","source":"**WordsTagsCount**\nFind 3 most popular tags and 3 most popular words in the train data"},{"metadata":{"trusted":true,"_uuid":"b8ab270c2b7f5d805ac879c2584db3062ac8cce3"},"cell_type":"code","source":"from collections import Counter\ntag_counts = Counter()\nword_counts = Counter()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e894595a9674bf23bd99c28d1c8c58b701283f42"},"cell_type":"code","source":"for sentence in X_train:\n    for words in sentence.split():\n        word_counts[words] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b62be25e0764d0a84fe3ca371b5228ca1d1bcfa"},"cell_type":"code","source":"for tag in y_train:\n    for l in tag:\n        tag_counts[l] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95bc7aaec6dbb47b53cb77a7601516ec765442c2"},"cell_type":"code","source":"print(tag_counts.most_common(5))\nprint(word_counts.most_common(5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3aae74f7e88a6c85c04db9bc40d042a123dc6b8"},"cell_type":"markdown","source":"**Transforming text to a vector***\nMachine Learning algorithms work with numeric data and we cannot use the provided text data \"as is\". There are many ways to transform text data to numeric vectors. In this task you will try to use two of them.\n\nBag of words\nOne of the well-known approaches is a bag-of-words representation. To create this transformation, follow the steps:\n\nFind N most popular words in train corpus and numerate them. Now we have a dictionary of the most popular words.\nFor each title in the corpora create a zero vector with the dimension equals to N.\nFor each text in the corpora iterate over words which are in the dictionary and increase by 1 the corresponding coordinate."},{"metadata":{"trusted":true,"_uuid":"a8cd24cda68e99b9cd611063192027a70f7c7728"},"cell_type":"code","source":"# most common words sorted by count in decending order\nmost_common_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:6000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ae7a1c22f58b4c339acaea52e6da5040269abd6"},"cell_type":"code","source":"DICT_SIZE = 5000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40cd02b40a85d92cbb5b4a7538d3fd064a014b4b"},"cell_type":"code","source":"WORDS_TO_INDEX = {p[0]:i for i,p in enumerate(most_common_words[:DICT_SIZE])}\nINDEX_TO_WORDS = {WORDS_TO_INDEX[k]:k for k in WORDS_TO_INDEX}\nALL_WORDS = WORDS_TO_INDEX.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e03cb2f9d411d6757240b147956aa2213fe52ef"},"cell_type":"code","source":"def my_bag_of_words(text, words_to_index, dict_size):\n    \"\"\"\n        text: a string\n        dict_size: size of the dictionary\n        return a vector which is a bag-of-words representation of 'text'\n    \"\"\"\n    result_vector = np.zeros(dict_size) # create vector with all zeroes\n    for word in text.split():\n        if word in words_to_index:\n            result_vector[words_to_index[word]] += 1\n    return result_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e444acec703828cbb5cb19d1395031205f073f8d"},"cell_type":"code","source":"def test_my_bag_of_words():\n    words_to_index = {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n    examples = ['hi how are you']\n    answers = [[1, 1, 0, 1]]\n    for ex, ans in zip(examples, answers):\n        if (my_bag_of_words(ex, words_to_index, 4) != ans).any():\n            return \"Wrong answer for the case: '%s'\" % ex\n    return 'Basic tests are passed.'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25ca384e161a363419c1ae59dfd73761b9172697"},"cell_type":"code","source":"print(test_my_bag_of_words())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c428086da74472be66df79bb0f60c633de9a1f96"},"cell_type":"code","source":"from scipy import sparse as sp_sparse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68635da3f9caad2e40070515f0e1271a0b0ca943"},"cell_type":"code","source":"X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\nX_val_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_val])\nprint('X_train shape ', X_train_mybag.shape)\nprint('X_val shape ', X_val_mybag.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4dbd285a644631e90f8b8bc793a64a1db833d87"},"cell_type":"markdown","source":"**TF-IDF**\nThe second approach extends the bag-of-words framework by taking into account total frequencies of words in the corpora. It helps to penalize too frequent words and provide better features space.\n\nImplement function tfidf_features using class TfidfVectorizer from scikit-learn. Use train corpus to train a vectorizer. Don't forget to take a look into the arguments that you can pass to it. We suggest that you filter out too rare words (occur less than in 5 titles) and too frequent words (occur more than in 90% of the titles). Also, use bigrams along with unigrams in your vocabulary."},{"metadata":{"trusted":true,"_uuid":"a650bcc0d993973e33653ad1960f3739466d98a8"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68e9a90d3a57e557e6a49f0a64dd680d47bf5669"},"cell_type":"code","source":"def tfidf_features(X_train, X_val):\n    \"\"\"\n        X_train, X_val, X_test — samples        \n        return TF-IDF vectorized representation of each sample and vocabulary\n    \"\"\"  \n    tfidf_vectorizer = TfidfVectorizer(token_pattern='(\\S+)', min_df=5, max_df=0.9, ngram_range=(1,2))\n    #/s represent white space where as + stands for one or more\n    # consider one or more white space for splitting\n    tfidf_vectorizer.fit(X_train)\n    X_train = tfidf_vectorizer.transform(X_train)\n    X_val = tfidf_vectorizer.transform(X_val)\n    return X_train, X_val, tfidf_vectorizer.vocabulary_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a8e5dc83a1ddc61409237d02a0c093c245cf54b"},"cell_type":"code","source":"X_train_tfidf, X_val_tfidf, tfidf_vocab = tfidf_features(X_train, X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"443055bacc4e6a572bce4515efa580db1d24ff4d"},"cell_type":"code","source":"tfidf_reversed_vocab = {i:word for word,i in tfidf_vocab.items()}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"500da118fcb65ffb24e970345e51feb913b27568"},"cell_type":"markdown","source":"Once you have done text preprocessing, always have a look at the results. Be very careful at this step, because the performance of future models will drastically depend on it.\n\nIn this case, check whether you have c++ or c# in your vocabulary, as they are obviously important tokens in our tags prediction task:"},{"metadata":{"trusted":true,"_uuid":"47a694815d022ab0a5243b73072179c56e56ec16"},"cell_type":"code","source":"print('c++' in tfidf_vocab)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2111963a22f34e01bd70e0ab2cfa6ac0f67c72c"},"cell_type":"markdown","source":"**MultiLabel classifier**\nAs we have noticed before, in this task each example can have multiple tags. To deal with such kind of prediction, we need to transform labels in a binary form and the prediction will be a mask of 0s and 1s. For this purpose it is convenient to use MultiLabelBinarizer from sklearn."},{"metadata":{"trusted":true,"_uuid":"7afa65189fb474aa7c48e698f6bae44174f72e9f"},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f82b63575ed026a76d2065221fe2d74ce1327896"},"cell_type":"code","source":"# to convert labels into one hot encoding\nfrom sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer(classes=sorted(tag_counts.keys()))\ny_train = mlb.fit_transform(y_train)\ny_val = mlb.fit_transform(y_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54d9371d8fa67c3f33d672c7605ddf735ce6cc73"},"cell_type":"markdown","source":"Implement the function train_classifier for training a classifier. In this task we suggest to use One-vs-Rest approach, which is implemented in OneVsRestClassifier class. In this approach k classifiers (= number of tags) are trained. As a basic classifier, use LogisticRegression. It is one of the simplest methods, but often it performs good enough in text classification tasks. It might take some time, because a number of classifiers to train is large"},{"metadata":{"trusted":true,"_uuid":"c1e76f83a2355a8c6e0ecc0bd38305fa2179b8f9"},"cell_type":"code","source":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b951705f3acfa20e9db9b54b19bb599c0e0d13f0"},"cell_type":"code","source":"def train_classifier(X_train, y_train):\n    logregss = LogisticRegression(C=3,penalty='l1')\n    onevrest = OneVsRestClassifier(logregss)\n    onevrest.fit(X_train,y_train)\n    return onevrest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30cac3e9560bc19a2170c12fbf9d9eee03ef68dd"},"cell_type":"code","source":"classifier_mybag = train_classifier(X_train_mybag, y_train)\nclassifier_tfidf = train_classifier(X_train_tfidf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bed2837b5f29750f2b508cbff89f2d107fb6d165"},"cell_type":"markdown","source":"Now you can create predictions for the data. You will need two types of predictions: labels and scores.\n\n"},{"metadata":{"trusted":true,"_uuid":"85eb0975ec205b46651d98212fe0ddc50471f1cb"},"cell_type":"code","source":"y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag)\ny_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)\n\ny_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\ny_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40cbc77176b1549b6ba6e4ea569f6feb62e46c83"},"cell_type":"code","source":"y_val_pred_inversed = mlb.inverse_transform(y_val_predicted_labels_tfidf)\ny_val_inversed = mlb.inverse_transform(y_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6a36d7e5734d840dd7c3467f096b6830c43cebf"},"cell_type":"markdown","source":"**Evaluation**\nTo evaluate the results we will use several classification metrics:\n\n* **Accuracy**\n* **F1-score**\n* **Area under ROC-curve**\n* **Area under precision-recall curve**\n\nMake sure you are familiar with all of them. How would you expect the things work for the multi-label scenario? Read about micro/macro/weighted averaging following the sklearn links provided above."},{"metadata":{"trusted":true,"_uuid":"c45473fc6876abf2ae63cf594ff495f4b57ce3b7"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score \nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import recall_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e7fec90108441030482900d3d48565bea85da51"},"cell_type":"code","source":"def print_evaluation_scores(y_val, predicted):\n    # print(len(y_val), len(y_val))\n    accuracy = accuracy_score(y_val, predicted)\n    print(#accuracy,\n          #f1_score(y_val, predicted, average='macro'),\n          #f1_score(y_val, predicted, average='micro'),\n          f1_score(y_val, predicted, average='weighted')#,\n#           average_precision_score(y_val, predicted, average='macro')\n#           average_precision_score(y_val, predicted, average='micro'),\n          #average_precision_score(y_val, predicted, average='weighted')\n         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dc125008c1d9fa1cdb07d52284fdabf0dfad4ac"},"cell_type":"code","source":"print('Bag-of-words')\nprint_evaluation_scores(y_val, y_val_predicted_labels_mybag)\nprint('Tfidf')\nprint_evaluation_scores(y_val, y_val_predicted_labels_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ffe6526aa2a2325183957b8d94cbb7ba40e4def"},"cell_type":"markdown","source":" ROC curve for the case of multi-label classification\n Provided function roc_auc can make it for you. The input parameters of this function are:\n\n* true labels\n* decision functions scores\n* number of classes"},{"metadata":{"_uuid":"877d40bf394d9e80321aeafb87d2ff4fb0dd5b99"},"cell_type":"markdown","source":"**Hyper parameter can be tuned best output i got with c = 3 & penalty = 'l1'**"},{"metadata":{"_uuid":"813500f1f5c890243128370ac89899ab9ac09bda"},"cell_type":"markdown","source":"**Analysis of the most important features\n**\n\nFinally, it is usually a good idea to look at the features (words or n-grams) that are used with the largest weigths in your logistic regression model.\n\nImplement the function print_words_for_tag to find them. Get back to sklearn documentation on OneVsRestClassifier and LogisticRegression if needed"},{"metadata":{"trusted":true,"_uuid":"a2750b5aa497a49d8a1ba07254e596031b70c708"},"cell_type":"code","source":"def print_words_for_tag(classifier, tag, tags_classes, index_to_words, all_words):\n    \"\"\"\n        classifier: trained classifier\n        tag: particular tag\n        tags_classes: a list of classes names from MultiLabelBinarizer\n        index_to_words: index_to_words transformation\n        all_words: all words in the dictionary\n        \n        return nothing, just print top 5 positive and top 5 negative words for current tag\n    \"\"\"\n    print('Tag:\\t{}'.format(tag))\n    \n    idx = tags_classes.index(tag)\n    coef=classifier_tfidf.coef_[idx]\n    cd = {i:coef[i] for i in range(len(coef))}\n    scd=sorted(cd.items(), key=lambda x: x[1], reverse=True)\n       \n    top_positive_words = [index_to_words[k[0]] for k in scd[:5]]# top-5 words sorted by the coefficiens.\n    top_negative_words = [index_to_words[k[0]] for k in scd[-5:]]# bottom-5 words  sorted by the coefficients.\n    print('Top positive words:\\t{}'.format(', '.join(top_positive_words)))\n    print('Top negative words:\\t{}\\n'.format(', '.join(top_negative_words)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d851005d74fe798135a682e588fa5dab5804d1f"},"cell_type":"code","source":"print_words_for_tag(classifier_tfidf, 'c', mlb.classes, tfidf_reversed_vocab, ALL_WORDS)\nprint_words_for_tag(classifier_tfidf, 'c++', mlb.classes, tfidf_reversed_vocab, ALL_WORDS)\nprint_words_for_tag(classifier_tfidf, 'linux', mlb.classes, tfidf_reversed_vocab, ALL_WORDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9ed6b6d4544c80fb759deeb4c3712a256624485"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}